import functools
import glob
import json
import logging
import math
import os
import pickle
from collections import defaultdict
from pathlib import Path

import networkx as nx
import numpy as np
from sklearn.neighbors import BallTree
from tqdm import tqdm

# --- å®šæ•°å®šç¾© ---
CACHE_DIR = os.path.join(os.path.dirname(__file__), "../datas/geometry_cache")
ORIGINAL_PATHS_DIR = os.path.join(os.path.dirname(__file__), "../datas/paths")
OUTPUT_PATHS_DIR = os.path.join(os.path.dirname(__file__), "../datas/paths_merged")

EPSILON_H_METERS = 80
EPSILON_V_METERS = 50
EARTH_RADIUS_METERS = 6371000
FILTER_MAX_SHORT_PATH_LENGTH_METERS = 500
FILTER_MAX_FLAT_ELEV_DIFF_METERS = 20

# --- ãƒ­ã‚°è¨­å®š ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()],
)
log = logging.getLogger(__name__)


def haversine(lat1, lon1, lat2, lon2):
    """2ç‚¹é–“ã®å¤§å††è·é›¢ã‚’è¨ˆç®—"""
    # ç·¯åº¦çµŒåº¦ã‚’ãƒ©ã‚¸ã‚¢ãƒ³ã«å¤‰æ›
    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    # ãƒãƒ¼ãƒãƒ¼ã‚µã‚¤ãƒ³å…¬å¼ã‚’ä½¿ç”¨ã—ã¦å¤§å††è·é›¢ã‚’è¨ˆç®—
    a = (
        math.sin(dlat / 2) ** 2
        + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2
    )
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
    return EARTH_RADIUS_METERS * c


def calculate_way_length(geometry):
    """çµŒè·¯ã®å…¨é•·ã‚’è¨ˆç®—"""
    total_length = 0
    # å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé–“ã®è·é›¢ã‚’ç´¯ç©
    for i in range(len(geometry) - 1):
        p1, p2 = geometry[i], geometry[i + 1]
        total_length += haversine(p1["lat"], p1["lon"], p2["lat"], p2["lon"])
    return total_length


@functools.lru_cache(maxsize=None)
def get_elevation(lat, lon, cache_dir="/app/datas/elevation_cache"):
    """æ¨™é«˜ã‚’å–å¾—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ« + ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰"""
    cache_key = f"{lat:.6f}_{lon:.6f}.pkl"
    cache_path = Path(cache_dir)
    cache_file = cache_path / cache_key

    try:
        with open(cache_file, "rb") as f:
            return pickle.load(f)
    except Exception as e:
        log.warning(f"Failed to load cache for {lat}, {lon}: {e}.")
        raise ValueError(f"Failed to load cache for {lat}, {lon}: {e}")


class UnionFind:
    """Union-Findï¼ˆç´ é›†åˆãƒ‡ãƒ¼ã‚¿æ§‹é€ ï¼‰"""

    def __init__(self, items):
        # å„è¦ç´ ã®è¦ªã¨æ·±ã•ã‚’åˆæœŸåŒ–
        self.parent = {item: item for item in items}
        self.rank = {item: 0 for item in items}

    def find(self, i):
        """ãƒ«ãƒ¼ãƒˆã‚’æ¤œç´¢ï¼ˆãƒ‘ã‚¹åœ§ç¸®ï¼‰"""
        if self.parent[i] == i:
            return i
        # çµŒè·¯åœ§ç¸®: è¦ªã‚’ç›´æ¥ãƒ«ãƒ¼ãƒˆã«è¨­å®š
        self.parent[i] = self.find(self.parent[i])
        return self.parent[i]

    def union(self, i, j):
        """é›†åˆã‚’ãƒãƒ¼ã‚¸ï¼ˆãƒ©ãƒ³ã‚¯ã«ã‚ˆã‚‹ä½µåˆï¼‰"""
        root_i = self.find(i)
        root_j = self.find(j)
        if root_i != root_j:
            # ãƒ©ãƒ³ã‚¯ã«åŸºã¥ã„ã¦æœ¨ã‚’ãƒãƒ¼ã‚¸
            if self.rank[root_i] < self.rank[root_j]:
                self.parent[root_i] = root_j
            elif self.rank[root_i] > self.rank[root_j]:
                self.parent[root_j] = root_i
            else:
                self.parent[root_j] = root_i
                self.rank[root_i] += 1
            return True
        return False

    def get_clusters(self):
        """å…¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’è¿”ã™"""
        clusters = defaultdict(list)
        # å„è¦ç´ ã‚’ãƒ«ãƒ¼ãƒˆã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        for item in self.parent:
            root = self.find(item)
            clusters[root].append(item)
        return clusters


def save_to_cache(key, data):
    """ã‚¸ã‚ªãƒ¡ãƒˆãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
    os.makedirs(CACHE_DIR, exist_ok=True)
    cache_file = os.path.join(CACHE_DIR, f"{key}.json")
    try:
        with open(cache_file, "w") as f:
            json.dump(data, f)
    except Exception as e:
        log.error(f"âŒ Failed to save cache {key}: {e}")


def load_from_cache(key):
    """ã‚¸ã‚ªãƒ¡ãƒˆãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿"""
    cache_file = os.path.join(CACHE_DIR, f"{key}.json")
    if os.path.exists(cache_file):
        try:
            with open(cache_file, "r") as f:
                return json.load(f)
        except Exception as e:
            log.warning(f"âš ï¸ Failed to load cache {key}, refetching...: {e}")
    return None


def filter_ways_and_endpoints(all_ways, all_endpoints):
    """çµŒè·¯ã¨ç«¯ç‚¹ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    filtered_ways = {}
    filtered_endpoints = []

    for way_id, way_data in tqdm(
        all_ways.items(),
        desc="Filtering ways and endpoints",
        total=len(all_ways),
        unit="way",
    ):
        geometry = way_data["geometry"]
        start_node = geometry[0]
        end_node = geometry[-1]
        # çµŒè·¯ã®ç›´ç·šè·é›¢ã‚’è¨ˆç®—
        way_length = calculate_way_length(geometry)
        # æ¨™é«˜å·®ã‚’è¨ˆç®—
        start_alt = get_elevation(start_node["lat"], start_node["lon"])
        end_alt = get_elevation(end_node["lat"], end_node["lon"])
        way_elev_diff = abs(start_alt - end_alt)

        # ä¸€å®šä»¥ä¸Šã®é•·ã•ã¾ãŸã¯æ¨™é«˜å·®ãŒã‚ã‚‹çµŒè·¯ã®ã¿ä¿æŒ
        if (
            way_length >= FILTER_MAX_SHORT_PATH_LENGTH_METERS
            or way_elev_diff >= FILTER_MAX_FLAT_ELEV_DIFF_METERS
        ):
            filtered_ways[way_id] = way_data
            filtered_endpoints.extend(
                [ep for ep in all_endpoints if ep["way_id"] == way_id]
            )

    log.info(
        f"âœ… Filtering complete: {len(filtered_ways)} ways, {len(filtered_endpoints)} endpoints retained."
    )
    return filtered_ways, filtered_endpoints


def process_json_file(f_path):
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰çµŒè·¯ã¨ç«¯ç‚¹ã‚’æŠ½å‡º"""
    try:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç¢ºèª
        cache_key = Path(f_path).stem
        cached_data = load_from_cache(cache_key)
        if cached_data:
            return cached_data["ways"], cached_data["endpoints"]

        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        with open(f_path, "r") as f:
            data = json.load(f)

        local_ways = {}
        local_endpoints = []

        # å„è¦ç´ ã‚’å‡¦ç†
        for element in data.get("elements", []):
            if element.get("type") == "way" and "geometry" in element:
                way_id = element["id"]
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                if way_id in local_ways:
                    continue

                geometry = element["geometry"]
                # ã‚¸ã‚ªãƒ¡ãƒˆãƒªã®å¦¥å½“æ€§ç¢ºèª
                if not geometry or len(geometry) < 2:
                    log.warning(f"âš ï¸ Skipping way {way_id}: Invalid geometry")
                    continue

                local_ways[way_id] = element

                # çµŒè·¯ã®å§‹ç‚¹ã¨çµ‚ç‚¹ã‚’å–å¾—ã—ã€æ¨™é«˜ã‚’ä»˜ä¸
                start_node = geometry[0]
                end_node = geometry[-1]
                start_alt = get_elevation(start_node["lat"], start_node["lon"])
                end_alt = get_elevation(end_node["lat"], end_node["lon"])

                # ç«¯ç‚¹æƒ…å ±ã‚’ä¿å­˜
                local_endpoints.append(
                    {
                        "id": f"{way_id}_start",
                        "way_id": way_id,
                        "is_start": True,
                        "lat": start_node["lat"],
                        "lon": start_node["lon"],
                        "alt": start_alt,
                    }
                )
                local_endpoints.append(
                    {
                        "id": f"{way_id}_end",
                        "way_id": way_id,
                        "is_start": False,
                        "lat": end_node["lat"],
                        "lon": end_node["lon"],
                        "alt": end_alt,
                    }
                )

        # å‡¦ç†çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        save_to_cache(cache_key, {"ways": local_ways, "endpoints": local_endpoints})
        return local_ways, local_endpoints
    except Exception as e:
        log.error(f"âŒ Failed to process file {f_path}: {e}")
        return {}, []


def phase_1_extract_endpoints(paths_dir):
    """Phase 1: çµŒè·¯ã¨ç«¯ç‚¹ã‚’æŠ½å‡º"""
    log.info("ğŸš€ Phase 1: Extracting endpoints...")
    all_ways = {}
    all_endpoints = []
    # å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    json_files = glob.glob(os.path.join(paths_dir, "*.json"))

    if not json_files:
        log.warning(f"ğŸ¤” No JSON files found in: {paths_dir}")
        return {}, []

    # é€æ¬¡å‡¦ç†ã§JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    for f in tqdm(json_files, desc="Processing JSON files", unit="file"):
        try:
            local_ways, local_endpoints = process_json_file(f)
            all_ways.update(local_ways)
            all_endpoints.extend(local_endpoints)
        except Exception as e:
            log.error(f"âŒ Failed to process file {f}: {e}")

    log.info(
        f"âœ… Phase 1 complete: {len(all_endpoints)} endpoints from {len(all_ways)} ways."
    )
    return all_ways, all_endpoints


def phase_2_cluster_junctions(all_endpoints, epsilon_h, epsilon_v):
    """Phase 2: ç«¯ç‚¹ã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°"""
    log.info("ğŸš€ Phase 2: Clustering junctions...")
    if not all_endpoints:
        log.warning("âš ï¸ No endpoints to cluster.")
        return None, {}

    endpoint_ids = [ep["id"] for ep in all_endpoints]
    uf = UnionFind(endpoint_ids)

    # BallTreeã§è¿‘å‚æ¤œç´¢ã‚’è¡Œã†ãŸã‚ã®åº§æ¨™é…åˆ—ã‚’ä½œæˆ
    log.info("Building BallTree coordinates...")
    endpoint_coords_rad = np.array(
        [[math.radians(ep["lat"]), math.radians(ep["lon"])] for ep in all_endpoints]
    )

    # BallTreeã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰
    log.info("Building BallTree index...")
    tree = BallTree(endpoint_coords_rad, metric="haversine")
    radius_rad = epsilon_h / EARTH_RADIUS_METERS

    # å„ç«¯ç‚¹ã®è¿‘å‚ã‚’æ¤œç´¢
    log.info("Querying BallTree for neighbors...")
    pairs_list = tree.query_radius(endpoint_coords_rad, r=radius_rad)
    log.info("Querying complete. Clustering...")

    # æ°´å¹³è·é›¢ã¨å‚ç›´è·é›¢ã®æ¡ä»¶ã‚’æº€ãŸã™ç«¯ç‚¹ãƒšã‚¢ã‚’æŠ½å‡º
    all_merge_pairs = []
    for i, neighbors in enumerate(
        tqdm(pairs_list, desc="Clustering endpoints", unit="endpoint")
    ):
        ep_i = all_endpoints[i]

        for j in neighbors:
            # è‡ªåˆ†è‡ªèº«ã‚„æ—¢ã«å‡¦ç†æ¸ˆã¿ã®ãƒšã‚¢ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if i >= j:
                continue

            ep_j = all_endpoints[j]

            # åŒã˜çµŒè·¯ã®ç«¯ç‚¹åŒå£«ã¯æ¥ç¶šã—ãªã„
            if ep_i["way_id"] == ep_j["way_id"]:
                continue

            # æ¨™é«˜å·®ãŒé–¾å€¤ä»¥å†…ã®å ´åˆã«ãƒãƒ¼ã‚¸å¯¾è±¡ã¨ã™ã‚‹
            if abs(ep_i["alt"] - ep_j["alt"]) < epsilon_v:
                all_merge_pairs.append((i, j))

    # Union-Findã‚’ä½¿ç”¨ã—ã¦ç«¯ç‚¹ã‚’ãƒãƒ¼ã‚¸
    log.info(f"Merging {len(all_merge_pairs)} endpoint pairs...")
    merge_count = 0
    for i, j in tqdm(all_merge_pairs, desc="Applying unions", unit="pair"):
        if uf.union(endpoint_ids[i], endpoint_ids[j]):
            merge_count += 1

    clusters = uf.get_clusters()
    log.info(
        f"âœ… Phase 2 complete: {len(endpoint_ids)} endpoints clustered into {len(clusters)} junctions ({merge_count} merges)."
    )

    # å„ç«¯ç‚¹ãŒã©ã®ã‚¯ãƒ©ã‚¹ã‚¿ã«å±ã™ã‚‹ã‹ã®ãƒãƒƒãƒ—ã‚’ä½œæˆ
    endpoint_to_cluster_map = {ep_id: uf.find(ep_id) for ep_id in endpoint_ids}
    return uf, endpoint_to_cluster_map


def phase_3_build_graph(all_ways, endpoint_to_cluster_map):
    """Phase 3: ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰"""
    log.info("ğŸš€ Phase 3: Building trail graph...")
    G = nx.MultiGraph()

    if not endpoint_to_cluster_map:
        log.warning("âš ï¸ No clusters found. Cannot build graph.")
        return G

    # å„çµŒè·¯ã‚’ã‚°ãƒ©ãƒ•ã®ã‚¨ãƒƒã‚¸ã¨ã—ã¦è¿½åŠ 
    for way_id, way_data in tqdm(all_ways.items(), desc="Building graph", unit="way"):
        start_ep_id = f"{way_id}_start"
        end_ep_id = f"{way_id}_end"

        # ç«¯ç‚¹ãŒå±ã™ã‚‹ã‚¯ãƒ©ã‚¹ã‚¿IDã‚’å–å¾—
        cluster_start_id = endpoint_to_cluster_map.get(start_ep_id)
        cluster_end_id = endpoint_to_cluster_map.get(end_ep_id)

        # ã‚¯ãƒ©ã‚¹ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if cluster_start_id is None or cluster_end_id is None:
            continue

        # ã‚¯ãƒ©ã‚¹ã‚¿é–“ã«ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ 
        G.add_edge(
            cluster_start_id,
            cluster_end_id,
            way_id=way_id,
            geometry=way_data["geometry"],
        )

    log.info(
        f"âœ… Phase 3 complete: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges."
    )
    return G


def phase_4_simplify_graph(G, endpoint_to_cluster_map):
    """Phase 4: ã‚°ãƒ©ãƒ•ã‚’ç°¡ç•¥åŒ–"""
    log.info("ğŸš€ Phase 4: Simplifying graph...")

    while True:
        # æ¬¡æ•°ãŒ2ã®ãƒãƒ¼ãƒ‰ï¼ˆä¸­é–“ç‚¹ï¼‰ã‚’æŠ½å‡º
        nodes_to_merge = [n for n, deg in G.degree() if deg == 2]

        if not nodes_to_merge:
            log.info("âœ… No more 2-degree nodes. Simplification complete.")
            break

        log.info(f"ğŸ”„ Found {len(nodes_to_merge)} 2-degree nodes to process.")
        merged_in_pass = 0

        for node in tqdm(nodes_to_merge, desc="Simplifying graph", unit="node"):
            # ãƒãƒ¼ãƒ‰ãŒå‰Šé™¤ã•ã‚Œã¦ã„ãªã„ã‹ã€æ¬¡æ•°ãŒ2ã§ã‚ã‚‹ã‹ç¢ºèª
            if node not in G or G.degree(node) != 2:
                continue

            neighbors = list(G.neighbors(node))
            if len(neighbors) != 2:
                continue

            n1, n2 = neighbors
            # è‡ªå·±ãƒ«ãƒ¼ãƒ—ã¯å‡¦ç†ã—ãªã„
            if n1 == n2:
                continue

            # éš£æ¥ã‚¨ãƒƒã‚¸ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            try:
                edge1_key = next(iter(G.get_edge_data(n1, node)))
                edge1_data = G.get_edge_data(n1, node)[edge1_key]
                edge2_key = next(iter(G.get_edge_data(node, n2)))
                edge2_data = G.get_edge_data(node, n2)[edge2_key]
            except StopIteration:
                log.warning(f"âš ï¸ Failed to get edge data for node {node}. Skipping.")
                continue

            geom1 = edge1_data["geometry"]
            way1_id = edge1_data["way_id"]
            geom2 = edge2_data["geometry"]
            way2_id = edge2_data["way_id"]

            # ã‚¸ã‚ªãƒ¡ãƒˆãƒªã®å‘ãã‚’èª¿æ•´
            way1_start_cluster = endpoint_to_cluster_map.get(f"{way1_id}_start")
            if way1_start_cluster is None:
                log.warning(f"Way {way1_id} not in map, skipping merge.")
                continue

            # ã‚¨ãƒƒã‚¸1ã®å‘ãã‚’èª¿æ•´
            ordered_geom1 = geom1 if way1_start_cluster == n1 else geom1[::-1]

            way2_start_cluster = endpoint_to_cluster_map.get(f"{way2_id}_start")
            if way2_start_cluster is None:
                log.warning(f"Way {way2_id} not in map, skipping merge.")
                continue

            # ã‚¨ãƒƒã‚¸2ã®å‘ãã‚’èª¿æ•´
            ordered_geom2 = geom2 if way2_start_cluster == node else geom2[::-1]

            # 2ã¤ã®ã‚¸ã‚ªãƒ¡ãƒˆãƒªã‚’çµåˆï¼ˆé‡è¤‡ã™ã‚‹ä¸­é–“ãƒãƒ¼ãƒ‰ã‚’é™¤ãï¼‰
            new_geometry = ordered_geom1 + ordered_geom2[1:]
            new_way_id = f"merged_{way1_id}_{way2_id}"

            # ä¸­é–“ãƒãƒ¼ãƒ‰ã‚’å‰Šé™¤ã—ã€æ–°ã—ã„ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ 
            G.remove_node(node)
            G.add_edge(n1, n2, way_id=new_way_id, geometry=new_geometry)

            # ãƒãƒƒãƒ”ãƒ³ã‚°æƒ…å ±ã‚’æ›´æ–°
            endpoint_to_cluster_map[f"{new_way_id}_start"] = n1
            endpoint_to_cluster_map[f"{new_way_id}_end"] = n2
            endpoint_to_cluster_map.pop(f"{way1_id}_start", None)
            endpoint_to_cluster_map.pop(f"{way1_id}_end", None)
            endpoint_to_cluster_map.pop(f"{way2_id}_start", None)
            endpoint_to_cluster_map.pop(f"{way2_id}_end", None)

            merged_in_pass += 1

        # 1å›ã®ãƒ‘ã‚¹ã§ãƒãƒ¼ã‚¸ãŒè¡Œã‚ã‚Œãªã‹ã£ãŸå ´åˆã¯çµ‚äº†
        if merged_in_pass == 0:
            log.info("    ... No merges were possible in this pass.")
            break

    log.info(
        f"âœ… Phase 4 complete: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges."
    )
    return G


def save_graph_to_json(G, output_dir, chunk_size):
    """ã‚°ãƒ©ãƒ•ã‚’JSONå½¢å¼ã§ä¿å­˜"""
    log.info(f"ğŸ’¾ Saving graph to {output_dir}...")
    elements = []
    unique_id_counter = 1

    # å„ã‚¨ãƒƒã‚¸ã‚’å‡¦ç†ã—ã¦JSONè¦ç´ ã‚’ä½œæˆ
    for u, v, data in tqdm(G.edges(data=True), desc="Processing edges", unit="edge"):
        geometry = data["geometry"]

        if not geometry:
            log.warning(f"âš ï¸ Skipping edge (u={u}, v={v}): Empty geometry.")
            continue

        # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—
        minlat = min(point["lat"] for point in geometry)
        maxlat = max(point["lat"] for point in geometry)
        minlon = min(point["lon"] for point in geometry)
        maxlon = max(point["lon"] for point in geometry)

        # æ¨™é«˜æƒ…å ±ãŒãªã„å ´åˆã¯å–å¾—
        if "alt" not in geometry[0]:
            try:
                lats = [point["lat"] for point in geometry]
                lons = [point["lon"] for point in geometry]
                # é€æ¬¡ã§æ¨™é«˜ã‚’å–å¾—
                altitudes = [get_elevation(lat, lon) for lat, lon in zip(lats, lons)]

                for i, point in enumerate(geometry):
                    point["alt"] = altitudes[i]
            except Exception as e:
                log.error(f"âŒ Failed to fetch altitudes for edge (u={u}, v={v}): {e}")
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
                for point in geometry:
                    point["alt"] = 0.0

        # æ¨™é«˜ãŒæ¬ è½ã—ã¦ã„ã‚‹ç‚¹ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
        for point in geometry:
            if "alt" not in point:
                point["alt"] = 0.0

        # JSONè¦ç´ ã‚’ä½œæˆ
        element = {
            "id": unique_id_counter,
            "bounds": {
                "minlat": minlat,
                "minlon": minlon,
                "maxlat": maxlat,
                "maxlon": maxlon,
            },
            "geometry": geometry,
        }
        elements.append(element)
        unique_id_counter += 1

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs(output_dir, exist_ok=True)
    # ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¦ä¿å­˜
    for i in tqdm(
        range(0, len(elements), chunk_size), desc="Saving chunks", unit="chunk"
    ):
        chunk = elements[i : i + chunk_size]
        output_file = os.path.join(
            output_dir, f"merged_trail_network_{i // chunk_size + 1}.json"
        )
        with open(output_file, "w") as f:
            json.dump({"elements": chunk}, f, indent=2)


if __name__ == "__main__":
    # Phase 1: çµŒè·¯ã¨ç«¯ç‚¹ã‚’æŠ½å‡º
    all_ways, all_endpoints = phase_1_extract_endpoints(ORIGINAL_PATHS_DIR)

    if all_ways:
        # all_ways, all_endpoints = filter_ways_and_endpoints(all_ways, all_endpoints)

        # Phase 2: ç«¯ç‚¹ã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        uf, endpoint_to_cluster_map = phase_2_cluster_junctions(
            all_endpoints, EPSILON_H_METERS, EPSILON_V_METERS
        )

        # Phase 3: ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰
        G = phase_3_build_graph(all_ways, endpoint_to_cluster_map)

        # Phase 4: ã‚°ãƒ©ãƒ•ã‚’ç°¡ç•¥åŒ–ï¼ˆå…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒã™ã‚‹ãŸã‚ã‚³ãƒ”ãƒ¼ã‚’ä½¿ç”¨ï¼‰
        G_copy = G.copy()
        endpoint_map_copy = endpoint_to_cluster_map.copy()
        G_simplified = phase_4_simplify_graph(G_copy, endpoint_map_copy)

        # çµæœã‚’ä¿å­˜
        if not os.path.exists(OUTPUT_PATHS_DIR):
            os.makedirs(OUTPUT_PATHS_DIR)
        save_graph_to_json(G_simplified, OUTPUT_PATHS_DIR, chunk_size=1024)
    else:
        log.error("âŒ No way data loaded. Exiting.")
