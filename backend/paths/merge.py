import glob
import json
import logging
import math
import os
import pickle
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
import networkx as nx
import numpy as np
from sklearn.neighbors import BallTree
from tqdm import tqdm
import sys  # sys ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# --- å®šæ•°å®šç¾© ---
# ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ«ãƒ€ã¨å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹
ORIGINAL_PATHS_DIR = os.path.join(os.path.dirname(__file__), "../datas/paths")
OUTPUT_PATHS_DIR = os.path.join(os.path.dirname(__file__), "../datas/paths_merged")

# è·é›¢ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®é–¾å€¤
EPSILON_H_METERS = 40  # æ°´å¹³è·é›¢ã®é–¾å€¤
EPSILON_V_METERS = 20  # å‚ç›´è·é›¢ã®é–¾å€¤
EARTH_RADIUS_METERS = 6371000  # åœ°çƒã®åŠå¾„
FILTER_MAX_SHORT_PATH_LENGTH_METERS = 200  # çŸ­ã„çµŒè·¯ã®æœ€å¤§é•·
FILTER_MAX_FLAT_ELEV_DIFF_METERS = 15  # å¹³å¦ãªçµŒè·¯ã®æœ€å¤§æ¨™é«˜å·®

# --- ãƒ­ã‚°è¨­å®š ---
# ãƒ­ã‚°ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¨å‡ºåŠ›è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()],
)
log = logging.getLogger(__name__)

# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
def haversine(lat1, lon1, lat2, lon2):
    """
    2ç‚¹é–“ã®å¤§å††è·é›¢ã‚’è¨ˆç®—ã™ã‚‹
    """
    # ç·¯åº¦çµŒåº¦ã‚’ãƒ©ã‚¸ã‚¢ãƒ³ã«å¤‰æ›
    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])

    # ãƒãƒ¼ã‚µã‚¤ãƒ³å…¬å¼
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = (
        math.sin(dlat / 2) ** 2
        + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2
    )
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
    distance = EARTH_RADIUS_METERS * c
    return distance


def calculate_way_length(geometry):
    """
    çµŒè·¯ã®å…¨é•·ã‚’è¨ˆç®—ã™ã‚‹
    """
    total_length = 0
    for i in range(len(geometry) - 1):
        p1 = geometry[i]
        p2 = geometry[i + 1]
        total_length += haversine(p1["lat"], p1["lon"], p2["lat"], p2["lon"])
    return total_length


def get_elevation(lat, lon, cache_dir="/app/datas/elevation_cache"):
    """
    ç·¯åº¦ãƒ»çµŒåº¦ã‹ã‚‰æ¨™é«˜ã‚’å–å¾—ã™ã‚‹ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰

    Args:
        lat: ç·¯åº¦
        lon: çµŒåº¦
        dem_data: DEMãƒ‡ãƒ¼ã‚¿
        cache_dir: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

    Returns:
        float: æ¨™é«˜ãƒ‡ãƒ¼ã‚¿
    """
    cache_key = f"{lat:.6f}_{lon:.6f}.pkl"
    cache_path = Path(cache_dir)
    cache_file = cache_path / cache_key

    if cache_file.exists():
        try:
            with open(cache_file, "rb") as f:
                return pickle.load(f)
        except Exception as e:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ãŸå ´åˆã¯ã€ã‚¨ãƒ©ãƒ¼ã‚’ãƒ­ã‚°ã«è¨˜éŒ²ã—ã€APIã‚’å‘¼ã³å‡ºã™
            log.warning(f"Failed to load cache for {lat}, {lon}: {e}. Refetching.")
            raise ValueError(f"Failed to load cache for {lat}, {lon}: {e}")

class UnionFind:
    """
    Union-Findï¼ˆç´ é›†åˆãƒ‡ãƒ¼ã‚¿æ§‹é€ ï¼‰ã‚’å®Ÿè£…ã™ã‚‹
    """

    def __init__(self, items):
        self.parent = {item: item for item in items}
        self.rank = {item: 0 for item in items}

    def find(self, i):
        """è¦ç´ iã‚’å«ã‚€é›†åˆã®ãƒ«ãƒ¼ãƒˆã‚’è¦‹ã¤ã‘ã‚‹"""
        if self.parent[i] == i:
            return i
        self.parent[i] = self.find(self.parent[i])  # ãƒ‘ã‚¹åœ§ç¸®
        return self.parent[i]

    def union(self, i, j):
        """è¦ç´ iã¨jã‚’å«ã‚€é›†åˆã‚’ãƒãƒ¼ã‚¸"""
        root_i = self.find(i)
        root_j = self.find(j)
        if root_i != root_j:
            # ãƒ©ãƒ³ã‚¯ã«ã‚ˆã‚‹ä½µåˆ
            if self.rank[root_i] < self.rank[root_j]:
                self.parent[root_i] = root_j
            elif self.rank[root_i] > self.rank[root_j]:
                self.parent[root_j] = root_i
            else:
                self.parent[root_j] = root_i
                self.rank[root_i] += 1
            return True
        return False

    def get_clusters(self):
        """å…¨ã¦ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’ {ãƒ«ãƒ¼ãƒˆ: [ãƒ¡ãƒ³ãƒãƒ¼]} å½¢å¼ã§è¿”ã™"""
        clusters = defaultdict(list)
        for item in self.parent:
            root = self.find(item)
            clusters[root].append(item)
        return clusters


# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
CACHE_DIR = os.path.join(os.path.dirname(__file__), "../datas/geometry_cache")

def save_to_cache(key, data):
    """
    ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹
    """
    os.makedirs(CACHE_DIR, exist_ok=True)
    cache_file = os.path.join(CACHE_DIR, f"{key}.json")
    with open(cache_file, "w") as f:
        json.dump(data, f)


def load_from_cache(key):
    """
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
    """
    cache_file = os.path.join(CACHE_DIR, f"{key}.json")
    if os.path.exists(cache_file):
        with open(cache_file, "r") as f:
            return json.load(f)
    return None


def process_json_file(f_path):
    """
    JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦çµŒè·¯ã¨ç«¯ç‚¹ã‚’æŠ½å‡ºã™ã‚‹
    """
    try:
        cache_key = Path(f_path).stem
        cached_data = load_from_cache(cache_key)
        if cached_data:
            return cached_data["ways"], cached_data["endpoints"]

        with open(f_path, "r") as f:
            data = json.load(f)

        local_ways = {}
        local_endpoints = []

        for element in data.get("elements", []):
            if element.get("type") == "way" and "geometry" in element:
                way_id = element["id"]
                if way_id in local_ways:
                    continue  # Skip duplicate ways

                geometry = element["geometry"]
                if not geometry or len(geometry) < 2:
                    log.warning(f"âš ï¸ Skipping way {way_id}: Invalid geometry")
                    continue

                # Get start and end nodes
                start_node = geometry[0]
                end_node = geometry[-1]

                # Get elevation
                start_alt = get_elevation(
                    start_node["lat"], start_node["lon"]
                )
                end_alt = get_elevation(end_node["lat"], end_node["lon"])

                local_ways[way_id] = element

                # Assign unique IDs to endpoints
                endpoint_id_start = f"{way_id}_start"
                endpoint_id_end = f"{way_id}_end"

                local_endpoints.append(
                    {
                        "id": endpoint_id_start,
                        "way_id": way_id,
                        "is_start": True,
                        "lat": start_node["lat"],
                        "lon": start_node["lon"],
                        "alt": start_alt,
                    }
                )
                local_endpoints.append(
                    {
                        "id": endpoint_id_end,
                        "way_id": way_id,
                        "is_start": False,
                        "lat": end_node["lat"],
                        "lon": end_node["lon"],
                        "alt": end_alt,
                    }
                )

        # Save to cache
        save_to_cache(cache_key, {"ways": local_ways, "endpoints": local_endpoints})
        return local_ways, local_endpoints
    except Exception as e:
        log.error(f"âŒ Failed to process file {f_path}: {e}")
        return {}, []


def filter_ways_and_endpoints(all_ways, all_endpoints, num_threads=4):
    """
    çµŒè·¯ã¨ç«¯ç‚¹ã‚’ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹
    """
    filtered_ways = {}
    filtered_endpoints = []

    def filter_way(way_id, way_data):
        """
        å˜ä¸€ã®çµŒè·¯ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹
        """
        geometry = way_data["geometry"]
        way_length = calculate_way_length(geometry)

        start_node = geometry[0]
        end_node = geometry[-1]
        start_alt = get_elevation(start_node["lat"], start_node["lon"])
        end_alt = get_elevation(end_node["lat"], end_node["lon"])
        way_elev_diff = abs(start_alt - end_alt)

        if (
            way_length >= FILTER_MAX_SHORT_PATH_LENGTH_METERS
            or way_elev_diff >= FILTER_MAX_FLAT_ELEV_DIFF_METERS
        ):
            return way_id, way_data, [
                ep for ep in all_endpoints if ep["way_id"] == way_id
            ]
        return None, None, []

    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        futures = {
            executor.submit(filter_way, way_id, way_data): way_id
            for way_id, way_data in all_ways.items()
        }

        for future in tqdm(
            as_completed(futures),
            desc="Filtering ways and endpoints",
            total=len(futures),
            unit="way",
        ):
            way_id, way_data, endpoints = future.result()
            if way_id and way_data:
                filtered_ways[way_id] = way_data
                filtered_endpoints.extend(endpoints)

    return filtered_ways, filtered_endpoints


def phase_1_extract_endpoints(paths_dir, num_threads=4):
    """
    ãƒ•ã‚§ãƒ¼ã‚º1: JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰çµŒè·¯ã¨ç«¯ç‚¹ã‚’æŠ½å‡ºã™ã‚‹
    """
    log.info("ğŸš€ Phase 1: Extracting endpoints...")
    all_ways = {}
    all_endpoints = []
    json_files = glob.glob(os.path.join(paths_dir, "*.json"))

    if not json_files:
        log.warning(f"ğŸ¤” No JSON files found in: {paths_dir}")
        return {}, []

    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        future_to_file = {executor.submit(process_json_file, f): f for f in json_files}
        for future in tqdm(
            as_completed(future_to_file),
            desc="Processing JSON files",
            total=len(json_files),
            unit="file",
        ):
            local_ways, local_endpoints = future.result()
            all_ways.update(local_ways)
            all_endpoints.extend(local_endpoints)

    log.info(f"âœ… Phase 1 complete: {len(all_endpoints)} endpoints from {len(all_ways)} ways.")
    return all_ways, all_endpoints


def phase_2_cluster_junctions(all_endpoints, epsilon_h, epsilon_v):
    """
    ãƒ•ã‚§ãƒ¼ã‚º2: ç«¯ç‚¹ã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹
    """
    log.info("ğŸš€ Phase 2: Clustering junctions...")
    if not all_endpoints:
        log.warning("âš ï¸ No endpoints to cluster.")
        return None, {}

    endpoint_ids = [ep["id"] for ep in all_endpoints]
    uf = UnionFind(endpoint_ids)

    # BallTreeç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ï¼ˆãƒ©ã‚¸ã‚¢ãƒ³å˜ä½ï¼‰
    endpoint_coords_rad = np.array(
        [[math.radians(ep["lat"]), math.radians(ep["lon"])] for ep in all_endpoints]
    )

    # ç©ºé–“ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰
    tree = BallTree(endpoint_coords_rad, metric="haversine")

    # æ°´å¹³é–¾å€¤ã‚’ãƒ©ã‚¸ã‚¢ãƒ³ã«å¤‰æ›
    radius_rad = epsilon_h / EARTH_RADIUS_METERS

    # æœ¨ã‚’ã‚¯ã‚¨ãƒªã—ã¦ã€æ°´å¹³åŠå¾„å†…ã®å…¨ãƒšã‚¢ã‚’è¦‹ã¤ã‘ã‚‹
    pairs_list = tree.query_radius(endpoint_coords_rad, r=radius_rad)

    merge_count = 0
    for i, neighbors in tqdm(
        enumerate(pairs_list),
        desc="Clustering endpoints",
        total=len(pairs_list),
        unit="endpoint",
    ):
        ep_i = all_endpoints[i]

        for j in neighbors:
            # è‡ªå·±ãƒ«ãƒ¼ãƒ—ã¨é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚’å›é¿
            if i >= j:
                continue

            ep_j = all_endpoints[j]

            # åŒã˜çµŒè·¯ã®ç«¯ç‚¹ã¯ãƒãƒ¼ã‚¸ã—ãªã„
            if ep_i["way_id"] == ep_j["way_id"]:
                continue

            # å‚ç›´è·é›¢ã‚’ãƒã‚§ãƒƒã‚¯
            if abs(ep_i["alt"] - ep_j["alt"]) < epsilon_v:
                # æ°´å¹³ãƒ»å‚ç›´ä¸¡æ–¹ã®ãƒã‚§ãƒƒã‚¯ã‚’é€šéã—ãŸå ´åˆã€ãƒ¦ãƒ‹ã‚ªãƒ³ã™ã‚‹
                if uf.union(ep_i["id"], ep_j["id"]):
                    merge_count += 1

    clusters = uf.get_clusters()
    log.info(f"âœ… Phase 2 complete: {len(endpoint_ids)} endpoints clustered into {len(clusters)} junctions.")

    # endpoint_id -> cluster_root_id ã®å˜ç´”ãªãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
    endpoint_to_cluster_map = {ep_id: uf.find(ep_id) for ep_id in endpoint_ids}

    return uf, endpoint_to_cluster_map


def phase_3_build_graph(all_ways, endpoint_to_cluster_map):
    """
    ãƒ•ã‚§ãƒ¼ã‚º3: çµŒè·¯ã¨ã‚¯ãƒ©ã‚¹ã‚¿ã‹ã‚‰ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ã™ã‚‹
    """
    log.info("ğŸš€ Phase 3: Building trail graph...")
    # MultiGraphã‚’ä½¿ç”¨ã—ã¦ã€åŒã˜2ã¤ã®ã‚¸ãƒ£ãƒ³ã‚¯ã‚·ãƒ§ãƒ³é–“ã«
    # è¤‡æ•°ã®ã‚¨ãƒƒã‚¸ï¼ˆä¾‹ï¼šå¤å­£ãƒ«ãƒ¼ãƒˆã¨å†¬å­£ãƒ«ãƒ¼ãƒˆï¼‰ã‚’è¨±å¯
    G = nx.MultiGraph()

    if not endpoint_to_cluster_map:
        log.warning("âš ï¸ No clusters found. Cannot build graph.")
        return G

    for way_id, way_data in tqdm(all_ways.items(), desc="Building graph", unit="way"):
        start_ep_id = f"{way_id}_start"
        end_ep_id = f"{way_id}_end"

        # çµŒè·¯ã®å§‹ç‚¹/çµ‚ç‚¹ãŒã©ã®ã‚¯ãƒ©ã‚¹ã‚¿ï¼ˆã‚¸ãƒ£ãƒ³ã‚¯ã‚·ãƒ§ãƒ³ï¼‰ã«å±ã™ã‚‹ã‹ã‚’ç‰¹å®š
        cluster_start_id = endpoint_to_cluster_map.get(start_ep_id)
        cluster_end_id = endpoint_to_cluster_map.get(end_ep_id)

        if cluster_start_id is None or cluster_end_id is None:
            log.warning(f"Skipping way {way_id}: Endpoint not found in cluster map.")
            continue

        # 2ã¤ã®ã‚¸ãƒ£ãƒ³ã‚¯ã‚·ãƒ§ãƒ³é–“ã«ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ 
        # ã‚¨ãƒƒã‚¸è‡ªä½“ãŒçµŒè·¯ã§ã‚ã‚‹ãŸã‚ã€ãã®ãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´
        G.add_edge(
            cluster_start_id,
            cluster_end_id,
            way_id=way_id,
            geometry=way_data["geometry"],
        )

    log.info(f"âœ… Phase 3 complete: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges.")
    return G


def phase_4_simplify_graph(G, endpoint_to_cluster_map):
    """
    ãƒ•ã‚§ãƒ¼ã‚º4: ã‚°ãƒ©ãƒ•ã‚’ç°¡ç•¥åŒ–ã™ã‚‹
    """
    log.info("ğŸš€ Phase 4: Simplifying graph...")

    # ãƒãƒ¼ã‚¸ã§ãã‚‹ãƒãƒ¼ãƒ‰ãŒãªããªã‚‹ã¾ã§ãƒ«ãƒ¼ãƒ—
    while True:
        nodes_to_merge = [n for n, deg in G.degree() if deg == 2]

        if not nodes_to_merge:
            log.info("âœ… No more 2-degree nodes. Simplification complete.")
            break

        log.info(f"ğŸ”„ Found {len(nodes_to_merge)} 2-degree nodes to process.")
        merged_in_pass = 0

        for node in tqdm(nodes_to_merge, desc="Simplifying graph", unit="node"):
            # ãƒãƒ¼ãƒ‰ãŒã¾ã å­˜åœ¨ã™ã‚‹ã‹ç¢ºèªï¼ˆãƒãƒ¼ã‚¸ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ï¼‰
            if node not in G or G.degree(node) != 2:
                continue

            # 2ã¤ã®éš£æ¥ãƒãƒ¼ãƒ‰ã‚’å–å¾—
            neighbors = list(G.neighbors(node))
            if len(neighbors) != 2:
                continue

            n1, n2 = neighbors

            # å˜ç´”ãªãƒ«ãƒ¼ãƒ—ï¼ˆn1 == n2ï¼‰ã¯ãƒãƒ¼ã‚¸ã—ãªã„
            if n1 == n2:
                continue

            # 2ã¤ã®ã‚¨ãƒƒã‚¸ã‚’å–å¾—ï¼ˆNetworkXã¯è¾æ›¸ã‚’è¿”ã™ãŸã‚ã€æœ€åˆã®ã‚¨ãƒƒã‚¸ã‚’å–å¾—ï¼‰
            # 2æ¬¡å…ƒãƒãƒ¼ãƒ‰ã‚»ã‚°ãƒ¡ãƒ³ãƒˆä¸Šã«å¹³è¡Œã‚¨ãƒƒã‚¸ãŒãªã„ã¨ä»®å®š
            edge1_key = next(iter(G.get_edge_data(n1, node)))
            edge1_data = G.get_edge_data(n1, node)[edge1_key]
            
            edge2_key = next(iter(G.get_edge_data(node, n2)))
            edge2_data = G.get_edge_data(node, n2)[edge2_key]

            geom1 = edge1_data["geometry"]
            way1_id = edge1_data["way_id"]
            geom2 = edge2_data["geometry"]
            way2_id = edge2_data["way_id"]

            # --- ã‚¸ã‚ªãƒ¡ãƒˆãƒªã®å‘ãã‚’ç‰¹å®š ---
            # geom1ã¨geom2ã‚’ã¤ãªãåˆã‚ã›ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
            # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®endpoint_to_cluster_mapã‚’ä½¿ç”¨ã—ã¦ã€
            # ã‚°ãƒ©ãƒ•ãƒãƒ¼ãƒ‰ã«å¯¾ã™ã‚‹ã‚¸ã‚ªãƒ¡ãƒˆãƒªã®ã€Œå‘ãã‚’çŸ¥ã‚‹ã€ã€‚

            way1_start_cluster = endpoint_to_cluster_map[f"{way1_id}_start"]

            # n1 -> node ã®é †ã§ã‚ã‚‹ ordered_geom1 ã‚’è¦‹ã¤ã‘ã‚‹
            if way1_start_cluster == n1:
                ordered_geom1 = geom1
            else:  # way1_end_cluster ã¯ n1 ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹
                ordered_geom1 = geom1[::-1]  # åè»¢

            way2_start_cluster = endpoint_to_cluster_map[f"{way2_id}_start"]

            # node -> n2 ã®é †ã§ã‚ã‚‹ ordered_geom2 ã‚’è¦‹ã¤ã‘ã‚‹
            if way2_start_cluster == node:
                ordered_geom2 = geom2
            else:  # way2_end_cluster ã¯ node ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹
                ordered_geom2 = geom2[::-1]  # åè»¢

            # ä¸­é–“ç‚¹ã®é‡è¤‡ã‚’é¿ã‘ã¦ã€ã‚¸ã‚ªãƒ¡ãƒˆãƒªã‚’ã¤ãªãåˆã‚ã›ã‚‹
            new_geometry = ordered_geom1 + ordered_geom2[1:]

            # ãƒãƒ¼ã‚¸ã•ã‚ŒãŸã“ã¨ã‚’ç¤ºã™æ–°ã—ã„ way_id ã‚’ä½œæˆ
            # ï¼ˆå®Ÿéš›ã®ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€IDã®ãƒªã‚¹ãƒˆãŒæœ›ã¾ã—ã„ã‹ã‚‚ã—ã‚Œãªã„ï¼‰
            new_way_id = f"merged_{way1_id}_{way2_id}"

            # ä¸­é–“ãƒãƒ¼ãƒ‰ã¨2ã¤ã®å¤ã„ã‚¨ãƒƒã‚¸ã‚’å‰Šé™¤
            G.remove_node(node)
            # æ–°ã—ã„ãƒãƒ¼ã‚¸ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ 
            G.add_edge(n1, n2, way_id=new_way_id, geometry=new_geometry)

            # --- endpoint_to_cluster_map ã®æ›´æ–° ---
            # æ–°ã—ã„çµŒè·¯ã®å§‹ç‚¹ã¨çµ‚ç‚¹ã‚’ãã‚Œãã‚Œã®ã‚¯ãƒ©ã‚¹ã‚¿ã«ãƒãƒƒãƒ”ãƒ³ã‚°
            # ã“ã‚Œã¯åå¾©ç°¡ç•¥åŒ–ã«é‡è¦
            endpoint_to_cluster_map[f"{new_way_id}_start"] = n1
            endpoint_to_cluster_map[f"{new_way_id}_end"] = n2
            
            # å¤ã„ way_id ã‚’ãƒãƒƒãƒ”ãƒ³ã‚°ã‹ã‚‰å‰Šé™¤
            del endpoint_to_cluster_map[f"{way1_id}_start"]
            del endpoint_to_cluster_map[f"{way1_id}_end"]
            del endpoint_to_cluster_map[f"{way2_id}_start"]
            del endpoint_to_cluster_map[f"{way2_id}_end"]


            merged_in_pass += 1

        if merged_in_pass == 0:
            log.info(
                "    ... No merges were possible in this pass (e.g., all 2-degree nodes were loops)."
            )
            break

    log.info(f"âœ… Phase 4 complete: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges.")
    return G


def save_graph_to_json(G, output_dir, chunk_size):
    """
    ã‚°ãƒ©ãƒ•ã‚’JSONå½¢å¼ã§ä¿å­˜ã™ã‚‹
    """
    log.info(f"ğŸ’¾ Saving graph to {output_dir}...")
    elements = []

    # ãƒ¦ãƒ‹ãƒ¼ã‚¯IDã‚«ã‚¦ãƒ³ã‚¿ã‚’åˆæœŸåŒ–
    unique_id_counter = 1

    for u, v, data in tqdm(G.edges(data=True), desc="Processing edges", unit="edge"):
        # ã‚¸ã‚ªãƒ¡ãƒˆãƒªã‚’æŠ½å‡ºã—ã€ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—
        geometry = data["geometry"]
        minlat = min(point["lat"] for point in geometry)
        maxlat = max(point["lat"] for point in geometry)
        minlon = min(point["lon"] for point in geometry)
        maxlon = max(point["lon"] for point in geometry)

        # å„ã‚¸ã‚ªãƒ¡ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆã«æ¨™é«˜ã‚’è¿½åŠ 
        for point in geometry:
            # alt ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèªï¼ˆå…ƒã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å¯èƒ½æ€§ãŒã‚ã‚‹ï¼‰
            # å­˜åœ¨ã—ãªã„å ´åˆã¯å–å¾—
            if "alt" not in point:
                point["alt"] = get_elevation(point["lat"], point["lon"])

        # çµŒè·¯ã«ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªæ•´æ•°IDã‚’å‰²ã‚Šå½“ã¦
        unique_id = unique_id_counter
        unique_id_counter += 1

        # è¦ç´ æ§‹é€ ã‚’ä½œæˆ
        element = {
            "id": unique_id,  # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªæ•´æ•°IDã‚’ä½¿ç”¨
            "bounds": {
                "minlat": minlat,
                "minlon": minlon,
                "maxlat": maxlat,
                "maxlon": maxlon,
            },
            "geometry": geometry,
        }
        elements.append(element)

    # è¦ç´ ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã€å„ãƒãƒ£ãƒ³ã‚¯ã‚’åˆ¥ã€…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    os.makedirs(output_dir, exist_ok=True)
    for i in tqdm(
        range(0, len(elements), chunk_size), desc="Saving chunks", unit="chunk"
    ):
        chunk = elements[i : i + chunk_size]
        output_file = os.path.join(
            output_dir, f"merged_trail_network_{i // chunk_size + 1}.json"
        )
        with open(output_file, "w") as f:
            json.dump({"elements": chunk}, f, indent=2)


# --- ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ ---

if __name__ == "__main__":
    # --- 4ãƒ•ã‚§ãƒ¼ã‚ºå‡¦ç†ã‚’å®Ÿè¡Œ ---
    import argparse

    parser = argparse.ArgumentParser(description="çµŒè·¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ãƒãƒ¼ã‚¸ã™ã‚‹")
    parser.add_argument(
        "--threads",
        type=int,
        default=4,
        help="ãƒ•ã‚§ãƒ¼ã‚º1ã§ä½¿ç”¨ã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰æ•°",
    )
    args = parser.parse_args()

    # ãƒ•ã‚§ãƒ¼ã‚º1: çµŒè·¯ã¨ç«¯ç‚¹ã‚’æŠ½å‡º
    all_ways, all_endpoints = phase_1_extract_endpoints(
        ORIGINAL_PATHS_DIR, num_threads=args.threads
    )

    if all_ways:
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ
        all_ways, all_endpoints = filter_ways_and_endpoints(all_ways, all_endpoints, num_threads=args.threads)

        # ãƒ•ã‚§ãƒ¼ã‚º2: ç«¯ç‚¹ã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        uf, endpoint_to_cluster_map = phase_2_cluster_junctions(
            all_endpoints, EPSILON_H_METERS, EPSILON_V_METERS
        )

        # ãƒ•ã‚§ãƒ¼ã‚º3: ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰
        G = phase_3_build_graph(all_ways, endpoint_to_cluster_map)

        # ãƒ•ã‚§ãƒ¼ã‚º4: ã‚°ãƒ©ãƒ•ã‚’ç°¡ç•¥åŒ–
        G_copy = G.copy()
        endpoint_map_copy = endpoint_to_cluster_map.copy()
        G_simplified = phase_4_simplify_graph(G_copy, endpoint_map_copy)

        # çµæœã‚’è¡¨ç¤º
        log.info("\n--- ğŸŒ² Final Merged Trail Network ğŸŒ² ---")
        log.info(f"Total Junctions (Nodes): {G_simplified.number_of_nodes()}")
        log.info(f"Total Segments (Edges): {G_simplified.number_of_edges()}")

        # çµæœã‚’ä¿å­˜
        if not os.path.exists(OUTPUT_PATHS_DIR):
            os.makedirs(OUTPUT_PATHS_DIR)
        save_graph_to_json(G_simplified, OUTPUT_PATHS_DIR, chunk_size=1024)
    else:
        log.error("âŒ No way data loaded. Exiting.")
