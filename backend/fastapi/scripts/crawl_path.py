import sys
import time
from pathlib import Path

import requests
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰ï¼‰
env_path = Path(__file__).parent.parent.parent / ".env"
load_dotenv(dotenv_path=env_path)

# FastAPIã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

from crud.path import create_path, get_path_by_osm_id
from schemas.path import PathImport
from sqlalchemy.orm import Session

ROOT_DIR = Path(__file__).parent.parent.parent
DATA_DIR = ROOT_DIR / "datas" / "paths"

# ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
DATA_DIR.mkdir(parents=True, exist_ok=True)

PATH = "https://overpass-api.de/api/interpreter"


def post_query(bottom: float, left: float, top: float, right: float):
    """æŒ‡å®šã•ã‚ŒãŸbboxã§Overpass APIã«ã‚¯ã‚¨ãƒªã‚’é€ä¿¡

    Args:
        bottom: æœ€å°ç·¯åº¦
        left: æœ€å°çµŒåº¦
        top: æœ€å¤§ç·¯åº¦
        right: æœ€å¤§çµŒåº¦

    Returns:
        dict | None: æˆåŠŸæ™‚ã¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã€å¤±æ•—æ™‚ã¯None
    """
    # Overpass APIã‚¯ã‚¨ãƒª
    query = f"""
[out:json][timeout:60];
(
  way["highway"="path"]({bottom},{left},{top},{right});
);
out geom;
"""

    try:
        print(f"  ğŸ”„ Fetching: bbox({bottom:.2f},{left:.2f},{top:.2f},{right:.2f})")
        response = requests.post(PATH, data={"data": query}, timeout=120)
        response.raise_for_status()

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
        data = response.json()
        elements_count = len(data.get("elements", []))
        print(f"  âœ… Fetched: {elements_count} elements")
        return data

    except requests.exceptions.Timeout:
        print("  â±ï¸  Timeout")
        return None
    except requests.exceptions.RequestException as e:
        print(f"  âŒ Error: {str(e)}")
        return None
    except Exception as e:
        print(f"  âŒ Unexpected error: {str(e)}")
        return None


def split_bbox(
    bbox: list[float], grid_size: float = 0.5
) -> list[tuple[float, float, float, float]]:
    """bboxã‚’å°ã•ãªã‚°ãƒªãƒƒãƒ‰ã«åˆ†å‰²

    Args:
        bbox: [right, left, bottom, top] å½¢å¼ã®bbox
        grid_size: ã‚°ãƒªãƒƒãƒ‰ã®ã‚µã‚¤ã‚ºï¼ˆç·¯åº¦ãƒ»çµŒåº¦ã®å·®ï¼‰

    Returns:
        [(bottom, left, top, right), ...] ã®ãƒªã‚¹ãƒˆ
    """
    right, left, bottom, top = bbox

    # çµŒåº¦æ–¹å‘ã®åˆ†å‰²æ•°
    lon_steps = int((right - left) / grid_size) + 1
    # ç·¯åº¦æ–¹å‘ã®åˆ†å‰²æ•°
    lat_steps = int((top - bottom) / grid_size) + 1

    grids = []
    for lat_i in range(lat_steps):
        for lon_i in range(lon_steps):
            grid_bottom = bottom + lat_i * grid_size
            grid_top = min(bottom + (lat_i + 1) * grid_size, top)
            grid_left = left + lon_i * grid_size
            grid_right = min(left + (lon_i + 1) * grid_size, right)

            grids.append((grid_bottom, grid_left, grid_top, grid_right))

    return grids


def rename_file_name(
    name: str,
    bbox: list[float],
    grid_size: float = 0.5,
):
    grids = split_bbox(bbox=bbox, grid_size=grid_size)
    total = len(grids)
    for i, (bottom, left, top, right) in enumerate(grids, 1):
        old_file = DATA_DIR / f"{name.replace(' ', '_')}_grid_{i}.json"
        new_file = (
            DATA_DIR / f"{name.replace(' ', '_')}_{bottom}_{left}_{top}_{right}.json"
        )
        if old_file.exists():
            old_file.rename(new_file)
            print(f"Renamed: {old_file} -> {new_file}")
        else:
            print(f"File not found, skipped: {old_file}")


def crawl_region(
    name: str,
    bbox: list[float],
    db: Session,
    grid_size: float = 0.5,
    delay: float = 2.0,
):
    """æŒ‡å®šã•ã‚ŒãŸåœ°åŸŸã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€ç›´æ¥DBã«ä¿å­˜

    Args:
        name: åœ°åŸŸå
        bbox: [right, left, bottom, top] å½¢å¼ã®bbox
        db: DBã‚»ãƒƒã‚·ãƒ§ãƒ³
        grid_size: ã‚°ãƒªãƒƒãƒ‰ã®ã‚µã‚¤ã‚º
        delay: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
    """
    print(f"\n{'=' * 60}")
    print(f"ğŸ—¾ Crawling: {name}")
    print(f"{'=' * 60}")

    grids = split_bbox(bbox, grid_size)
    total = len(grids)

    print(f"  ğŸ“Š Total grids: {total}")
    print(f"  ğŸ“ Grid size: {grid_size}Â° x {grid_size}Â°")
    print(f"  â±ï¸  Delay: {delay}s between requests\n")

    crawl_stats = {"total": total, "success": 0, "failed": 0}
    import_stats = {"total": 0, "created": 0, "skipped": 0, "errors": 0}

    for i, (bottom, left, top, right) in enumerate(grids, 1):
        print(f"[{i}/{total}] ", end="")

        if (DATA_DIR / f"{name.replace(' ', '_')}_grid_{i}.json").exists():
            print("  â­ï¸  Skipping (already exists)")
            crawl_stats["success"] += 1
            continue

        # APIã«ã‚¯ã‚¨ãƒªã‚’é€ä¿¡
        data = post_query(bottom, left, top, right)

        if data:
            crawl_stats["success"] += 1

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç›´æ¥DBã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            stats = import_data_to_db(data, db)
            for key in import_stats:
                import_stats[key] += stats[key]

            if stats["created"] > 0:
                print(f"    ğŸ’¾ Imported: {stats['created']} paths")

            with open(
                DATA_DIR / f"{name.replace(' ', '_')}_grid_{i}.json",
                "w",
                encoding="utf-8",
            ) as f:
                import json

                json.dump(data, f, ensure_ascii=False, indent=2)
        else:
            crawl_stats["failed"] += 1

        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼šå¾…æ©Ÿ
        if i < total:  # æœ€å¾Œã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å¾Œã¯å¾…ãŸãªã„
            time.sleep(delay)

    print(f"\n{'=' * 60}")
    print(f"ğŸ“Š {name} Summary")
    print(f"{'=' * 60}")
    print("  Crawl:")
    print(f"    Total grids: {crawl_stats['total']}")
    print(f"    âœ… Success: {crawl_stats['success']}")
    print(f"    âŒ Failed: {crawl_stats['failed']}")
    print("  Import:")
    print(f"    Total paths: {import_stats['total']}")
    print(f"    âœ… Created: {import_stats['created']}")
    print(f"    â­ï¸  Skipped: {import_stats['skipped']}")
    print(f"    âŒ Errors: {import_stats['errors']}")
    print(f"{'=' * 60}\n")

    return {"crawl": crawl_stats, "import": import_stats}


def import_data_to_db(data: dict, db: Session) -> dict:
    """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã®elementsã‚’DBã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

    Args:
        data: APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿
        db: DBã‚»ãƒƒã‚·ãƒ§ãƒ³

    Returns:
        ã‚¤ãƒ³ãƒãƒ¼ãƒˆçµæœã®çµ±è¨ˆæƒ…å ±
    """
    stats = {"total": 0, "created": 0, "skipped": 0, "errors": 0}

    try:
        elements = data.get("elements", [])
        # wayã‚¿ã‚¤ãƒ—ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
        elements = [e for e in elements if e.get("type") == "way"]
        stats["total"] = len(elements)

        for element in elements:
            try:
                # Pydanticã§ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
                path_import = PathImport(**element)

                # æ—¢å­˜ãƒã‚§ãƒƒã‚¯
                existing = get_path_by_osm_id(db, path_import.id)
                if existing:
                    stats["skipped"] += 1
                    continue

                # boundsã‚’dictå½¢å¼ã«å¤‰æ›
                bounds_dict = None
                if path_import.bounds:
                    bounds_dict = {
                        "minlat": path_import.bounds.minlat,
                        "minlon": path_import.bounds.minlon,
                        "maxlat": path_import.bounds.maxlat,
                        "maxlon": path_import.bounds.maxlon,
                    }

                # geometryã‚’dictå½¢å¼ã«å¤‰æ›
                geometries_dict = [
                    {"lat": g.lat, "lon": g.lon} for g in path_import.geometry
                ]

                # DBã«ä¿å­˜
                create_path(
                    db=db,
                    osm_id=path_import.id,
                    type=path_import.type,
                    bounds=bounds_dict,
                    nodes=path_import.nodes,
                    geometries=geometries_dict,
                    tags=path_import.tags,
                )

                stats["created"] += 1

            except Exception as e:
                stats["errors"] += 1
                print(
                    f"    âŒ Error importing OSM ID {element.get('id', 'unknown')}: {str(e)}"
                )
                db.rollback()

    except Exception as e:
        print(f"    âŒ Error processing data: {str(e)}")

    return stats


# åœ°åŸŸå®šç¾©ï¼ˆ[right, left, bottom, top]å½¢å¼ï¼‰
kyushuu = [132.18, 128.10, 30.9, 34.77]
honshuu = [142.1, 130.8, 33.05, 45.55]
hokkaido = [145.9, 139.8, 41, 45.5]


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸš€ Path Crawler & Importer - Overpass API")
    grid_size = 0.45
    rename_file_name("ä¹å·", kyushuu, grid_size=grid_size)
    rename_file_name("æœ¬å·", honshuu, grid_size=grid_size)
    rename_file_name("åŒ—æµ·é“", hokkaido, grid_size=0.45)

    # start_time = time.time()

    # # DBã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
    # db = SessionLocal()

    # try:
    #     # å„åœ°åŸŸã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ï¼ˆgrid_size=0.5ã§ã€é¢ç©ã¯ç´„0.25ï¼‰
    #     # 0.2ç¨‹åº¦ã«ã™ã‚‹ã«ã¯ã€grid_size=0.45ãã‚‰ã„ãŒè‰¯ã„ï¼ˆ0.45*0.45=0.2025ï¼‰

    #     all_crawl_stats = {"total": 0, "success": 0, "failed": 0}
    #     all_import_stats = {"total": 0, "created": 0, "skipped": 0, "errors": 0}

    #     # ä¹å·
    #     stats = crawl_region("ä¹å·", kyushuu, db, grid_size=grid_size, delay=0.1)
    #     for key in all_crawl_stats:
    #         all_crawl_stats[key] += stats["crawl"][key]
    #     for key in all_import_stats:
    #         all_import_stats[key] += stats["import"][key]

    #     # æœ¬å·
    #     stats = crawl_region("æœ¬å·", honshuu, db, grid_size=grid_size, delay=0.1)
    #     for key in all_crawl_stats:
    #         all_crawl_stats[key] += stats["crawl"][key]
    #     for key in all_import_stats:
    #         all_import_stats[key] += stats["import"][key]

    #     # åŒ—æµ·é“
    #     stats = crawl_region("åŒ—æµ·é“", hokkaido, db, grid_size=grid_size, delay=0.1)
    #     for key in all_crawl_stats:
    #         all_crawl_stats[key] += stats["crawl"][key]
    #     for key in all_import_stats:
    #         all_import_stats[key] += stats["import"][key]

    #     elapsed_time = time.time() - start_time

    #     # æœ€çµ‚ã‚µãƒãƒªãƒ¼
    #     print("\n" + "=" * 60)
    #     print("ğŸ‰ FINAL SUMMARY")
    #     print("=" * 60)
    #     print("  Crawl:")
    #     print(f"    Total grids: {all_crawl_stats['total']}")
    #     print(f"    âœ… Success: {all_crawl_stats['success']}")
    #     print(f"    âŒ Failed: {all_crawl_stats['failed']}")
    #     print("  Import:")
    #     print(f"    Total paths: {all_import_stats['total']}")
    #     print(f"    âœ… Created: {all_import_stats['created']}")
    #     print(f"    â­ï¸  Skipped: {all_import_stats['skipped']}")
    #     print(f"    âŒ Errors: {all_import_stats['errors']}")
    #     print(f"  â±ï¸  Total time: {elapsed_time:.2f}s ({elapsed_time / 60:.2f}min)")
    #     print("=" * 60)

    # finally:
    #     db.close()


if __name__ == "__main__":
    main()
