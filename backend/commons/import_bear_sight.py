import os
import sys
from datetime import datetime, timedelta
from pathlib import Path
import json
import hashlib

import httpx
from dotenv import load_dotenv

# Djangoã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
sys.path.insert(0, str(Path(__file__).parent.parent))
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "collectmap.settings")

import django

django.setup()

from bear.call_openai import analyze_article_with_llm
from bear.models import BearSighting

from commons.utils import get_coordinates_for_location

load_dotenv()

# LLMã¨DBçµæœã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š
LLM_CACHE_DIR = Path(__file__).parent.parent / "datas" / "bears_cache" / "llm"
DB_CACHE_DIR = Path(__file__).parent.parent / "datas" / "bears_cache" / "db"
LLM_CACHE_DIR.mkdir(parents=True, exist_ok=True)
DB_CACHE_DIR.mkdir(parents=True, exist_ok=True)

# NewsAPIè¨­å®š
# ã‚¯ãƒé–¢é€£ã®NHKè¨˜äº‹ã‚’éå»30æ—¥åˆ†å–å¾—
BASE_NEWS_API_URL = "https://newsapi.org/v2/everything"

NEWS_API_KEY = os.environ.get("NEWS_API_KEY")
if NEWS_API_KEY is None:
    raise ValueError("NEWS_API_KEY is not set in environment variables.")


def fetch_news_from_api() -> list[dict]:
    """NewsAPIã‚’ä½¿ç”¨ã—ã¦NHKã®ã‚¯ãƒé–¢é€£è¨˜äº‹ã‚’å–å¾—"""
    params = {
        "q": "ã‚¯ãƒ",
        "domains": "web.nhk",
        "from": (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d"),
        "sortBy": "publishedAt",
    }
    headers = {"Authorization": f"Bearer {NEWS_API_KEY}"}

    try:
        with httpx.Client() as client:
            response = client.get(BASE_NEWS_API_URL, params=params, headers=headers)
            response.raise_for_status()

            data = response.json()
            return data.get("articles", [])

    except httpx.HTTPStatusError as e:
        print(f"âŒ HTTP error while requesting NewsAPI: {e}")
        return []
    except Exception as e:
        print(f"âš ï¸ Unexpected error while requesting NewsAPI: {e}")
        return []


def get_cache_filename(url: str) -> str:
    """URLã®MD5ãƒãƒƒã‚·ãƒ¥ã‹ã‚‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ"""
    url_hash = hashlib.md5(url.encode()).hexdigest()
    return f"{url_hash}.json"


def load_llm_cache(url: str) -> dict | None:
    """LLMåˆ†æçµæœã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã‚€"""
    cache_file = LLM_CACHE_DIR / get_cache_filename(url)
    if cache_file.exists():
        try:
            with open(cache_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸ Error loading LLM cache: {e}")
    return None


def save_llm_cache(url: str, llm_result) -> None:
    """LLMåˆ†æçµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
    cache_file = LLM_CACHE_DIR / get_cache_filename(url)
    try:
        cache_data = {
            "url": url,
            "is_sighting": llm_result.is_sighting,
            "prefecture": llm_result.prefecture,
            "city": llm_result.city,
            "summary": llm_result.summary,
            "cached_at": datetime.now().isoformat(),
        }
        with open(cache_file, "w", encoding="utf-8") as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ Error saving LLM cache: {e}")


def load_db_cache(url: str) -> dict | None:
    """DBä¿å­˜ç”¨ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã‚€"""
    cache_file = DB_CACHE_DIR / get_cache_filename(url)
    if cache_file.exists():
        try:
            with open(cache_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸ Error loading DB cache: {e}")
    return None


def save_db_cache(url: str, sighting_data: dict) -> None:
    """DBä¿å­˜ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
    cache_file = DB_CACHE_DIR / get_cache_filename(url)
    try:
        cache_data = {
            "url": url,
            **sighting_data,
            "cached_at": datetime.now().isoformat(),
        }
        with open(cache_file, "w", encoding="utf-8") as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ Error saving DB cache: {e}")


def main():
    print(f"--- {datetime.now()} | Scheduled job started ---")

    # NewsAPIã‹ã‚‰è¨˜äº‹ã‚’å–å¾—
    articles = fetch_news_from_api()
    if not articles:
        print("No articles retrieved.")
        return

    print(f"Retrieved {len(articles)} articles. Starting analysis...")

    saved_count = 0
    for article in articles:
        url = article.get("url", "")
        if not url:
            continue
        
        # DBã«æ—¢å­˜ã®è¨˜äº‹ãŒãªã„ã‹ç¢ºèª
        existing = BearSighting.objects.filter(source_url=url).first()
        if existing:
            continue

        # DBã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç¢ºèª
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã‚Œã°LLMåˆ†æã¨ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—
        db_cache = load_db_cache(url)
        if db_cache:
            print(f"ğŸ“¦ Using cached DB result for: {url}")
            try:
                sighting = BearSighting(
                    prefecture=db_cache.get("prefecture", ""),
                    city=db_cache.get("city", ""),
                    latitude=db_cache.get("latitude", 0.0),
                    longitude=db_cache.get("longitude", 0.0),
                    summary=db_cache.get("summary", ""),
                    source_url=url,
                    image_url=db_cache.get("image_url", ""),
                    reported_at=datetime.fromisoformat(db_cache.get("reported_at", datetime.now().isoformat())),
                )
                sighting.save()
                saved_count += 1
                print(f"âœ… Saved bear sighting from cached data: {url}")
            except Exception as e:
                print(f"âŒ Error saving cached sighting: {e}")
            continue

        title = article.get("title", "")
        description = article.get("description", "")
        print(f"Analyzing article: {title}")

        # LLMã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç¢ºèª
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã‚Œã°LLM APIã®å‘¼ã³å‡ºã—ã‚’ã‚¹ã‚­ãƒƒãƒ—
        llm_cache = load_llm_cache(url)
        if llm_cache:
            print(f"ğŸ“¦ Using cached LLM result for: {title}")
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç–‘ä¼¼çš„ãªLLMçµæœã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
            class CachedResult:
                def __init__(self, data):
                    self.is_sighting = data.get("is_sighting", False)
                    self.prefecture = data.get("prefecture")
                    self.city = data.get("city")
                    self.summary = data.get("summary")
            llm_result = CachedResult(llm_cache)
        else:
            # LLMã§è¨˜äº‹ã‚’åˆ†æ
            llm_result = analyze_article_with_llm(title, description)
            if llm_result:
                save_llm_cache(url, llm_result)

        # ã‚¯ãƒã®ç›®æ’ƒæƒ…å ±ã§ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if not llm_result or not llm_result.is_sighting:
            continue

        # éƒ½é“åºœçœŒã¨å¸‚åŒºç”ºæ‘ã‹ã‚‰ç·¯åº¦çµŒåº¦ã‚’å–å¾—
        coordinates = get_coordinates_for_location(
            llm_result.prefecture, llm_result.city
        )
        try:
            # è¨˜äº‹ã®å…¬é–‹æ—¥æ™‚ã‚’å–å¾—
            reported_at = datetime.fromisoformat(
                article.get("publishedAt", datetime.now().isoformat()).replace(
                    "Z", "+00:00"
                )
            )
            
            # DBä¿å­˜ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            sighting_data = {
                "prefecture": llm_result.prefecture or "",
                "city": llm_result.city or "",
                "latitude": coordinates[0] if coordinates else 0.0,
                "longitude": coordinates[1] if coordinates else 0.0,
                "summary": llm_result.summary or "",
                "image_url": article.get("urlToImage", ""),
                "reported_at": reported_at.isoformat(),
            }
            
            # DBã«ä¿å­˜
            sighting = BearSighting(
                **{k: v for k, v in sighting_data.items() if k != "reported_at"},
                source_url=url,
                reported_at=reported_at,
            )
            sighting.save()
            
            # DBä¿å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            save_db_cache(url, sighting_data)
            
            saved_count += 1
            print(f"âœ… Saved bear sighting from article: {title}")
        except Exception as e:
            print(f"âŒ Error saving sighting from article '{title}': {e}")


if __name__ == "__main__":
    main()
